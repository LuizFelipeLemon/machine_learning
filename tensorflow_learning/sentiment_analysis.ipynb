{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMagzwGvoO1sn3Bwqp4v6KB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuizFelipeLemon/machine_learning/blob/master/tensorflow_learning/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8oNxCyYiYbW",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment analysis\n",
        "It is an example of sentiment analysis developed on top of the IMDb dataset. The IMDb dataset contains the text of 50,000 movie reviews from the Internet Movie Database. Each review is either positive or negative (for example, thumbs up or thumbs down). The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. Our goal is to build a classifier that is able to predict the binary judgment given the text. We can easily load IMDb via tf.keras and the sequences of words in the reviews have been converted to sequences of integers, where each integer represents a specific word in a dictionary. We also have a convenient way of padding sentences to max_len, so that we can use all sentences, whether short or long, as inputs to a neural network with an input vector of fixed size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zBzil2XiRjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500\n",
        "def load_data():\n",
        "        # Load data.\n",
        "        (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "        # Pad sequences with max_len.\n",
        "        X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "        X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "        return (X_train, y_train), (X_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikn8yduMilj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    # Input: - eEmbedding Layer.\n",
        "    # The model will take as input an integer matrix of size (batch,     # input_length).\n",
        "    # The model will output dimension (input_length, dim_embedding).\n",
        "    # The largest integer in the input should be no larger\n",
        "    # than n_words (vocabulary size).\n",
        "    model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Takes the maximum value of either feature vector from each of     # the n_words features.\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LttPz9JirsB",
        "colab_type": "code",
        "outputId": "3ad91dbf-2e64-4199-b809-c45dfd9a5e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "score = model.fit(X_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE, validation_data = (X_test, y_test))\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(\"\\nTest score:\", score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 24s 479ms/step - loss: 0.6719 - accuracy: 0.6300 - val_loss: 0.6309 - val_accuracy: 0.8368\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.4603 - accuracy: 0.8400 - val_loss: 0.3655 - val_accuracy: 0.8550\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.2798 - accuracy: 0.8864 - val_loss: 0.3074 - val_accuracy: 0.8722\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.2156 - accuracy: 0.9171 - val_loss: 0.2933 - val_accuracy: 0.8761\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 23s 467ms/step - loss: 0.1722 - accuracy: 0.9375 - val_loss: 0.2898 - val_accuracy: 0.8758\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 24s 471ms/step - loss: 0.1332 - accuracy: 0.9540 - val_loss: 0.2958 - val_accuracy: 0.8722\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 23s 466ms/step - loss: 0.1047 - accuracy: 0.9680 - val_loss: 0.3074 - val_accuracy: 0.8689\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 23s 465ms/step - loss: 0.0786 - accuracy: 0.9773 - val_loss: 0.3183 - val_accuracy: 0.8672\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.0593 - accuracy: 0.9846 - val_loss: 0.3385 - val_accuracy: 0.8622\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 24s 474ms/step - loss: 0.0450 - accuracy: 0.9891 - val_loss: 0.3544 - val_accuracy: 0.8594\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 24s 470ms/step - loss: 0.0335 - accuracy: 0.9924 - val_loss: 0.3719 - val_accuracy: 0.8587\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 24s 477ms/step - loss: 0.0243 - accuracy: 0.9954 - val_loss: 0.3894 - val_accuracy: 0.8554\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.0192 - accuracy: 0.9966 - val_loss: 0.4047 - val_accuracy: 0.8548\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 24s 474ms/step - loss: 0.0147 - accuracy: 0.9978 - val_loss: 0.4231 - val_accuracy: 0.8520\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 24s 474ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.4372 - val_accuracy: 0.8520\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.4536 - val_accuracy: 0.8499\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 24s 473ms/step - loss: 0.0079 - accuracy: 0.9993 - val_loss: 0.4640 - val_accuracy: 0.8506\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 23s 466ms/step - loss: 0.0081 - accuracy: 0.9986 - val_loss: 0.4771 - val_accuracy: 0.8500\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 23s 469ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.4892 - val_accuracy: 0.8494\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 24s 472ms/step - loss: 0.0059 - accuracy: 0.9991 - val_loss: 0.4994 - val_accuracy: 0.8501\n",
            "50/50 [==============================] - 2s 42ms/step - loss: 0.4994 - accuracy: 0.8501\n",
            "\n",
            "Test score: 0.4993669390678406\n",
            "Test accuracy: 0.8500800132751465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qkRkSHZi-TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}